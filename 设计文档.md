##### 数据处理流程设计文档

###### 系统目标
           设计这个系统的主要目标是用户只需要通过前端配置数据处理流程、后台就能完成用户定义的数据处理任务
       （Flink Job,MR Job）的自动装配和提交任务；在大部分情况下直接通过页面配置流程就能实现任务，提高程序的开发效率。
        数据处理流程这个概念太过模糊，在该项目中主要想针对数据ETL操作和特征工程两个方面的具体需求去实现
        
###### 设计要求
        1.能灵活的配置大部分数据流程
        2.可能需要完善的流程分析、这一点也是需要结合实际应用场景去积累通用的数据处理方式
        3.能将数据处理流程从数据处理框架中剥离出来，例如：可以用于Spark，Flink，Hadoop等都可以
        
        
###### 设计概要
       
          data-process-functional-service：
              主要是数据的转换操作，目前支持的是数据编码、解码、正则表达式、分割等功能、富化函数、规则引擎、
           其他数据源查询、自定义规则、子流程调用、指定某些命令在某些条件下跳过执行          
          
          设计方式：将这些一系列的数据变换操作抽象成Command对不同的Command可以进行不同方式的线性组合，
                    可以达到对数据不同处理流程的定义。通过构造不同的Command的方式向外提供可调用的API，
                    同时第三方可以按需求定义自己的Command满足业务要求。
        
          data-manage-service：这个组件主要是负责数据类型的定义（Schema的定义、字段的定义、存储策略等）和管理(数据类型的
         CURD、权限)、数据轨迹的管理（可以进行溯源）等方面的功能。定义好数据的管理程度决定着企业的数据和业务之间结合紧密
         度高低与否，所以良好的数据管理系统能促进企业的业务发展。
         设计功能规划：Schema的定义、字段的定义、存储策略等；数据类型的CURD、权限；数据轨迹的管理
           
           data-process-flow：这个组件是数据处理流程、是想描述一条数据或某个类型的数据经过哪些主要步骤的处理。
           它的设计主要是连接数据类型和数据处理作业之间的枢纽；
           它的设计要求比较高：1.需要考虑能扩展所有的不同作业方式的系统，例如：Spark、Flink、Hadoop等
                               2.目的就是通过对数据流程的定义，系统能自动生成该数据流程的作业任务。
                               最后的效果就是：用户在界面上定义好一套数据处理流程，选择Flink作业方式；
                               然后submit该Job到Flink集群上运行，用户就完成了作业的开发、运行。        
         
        关键点：
            1.是如何定义数据处理节点的模型、需要结合数据处理功能模块、作业系统的特点（Flink、Spark）等去普适这些框架
            2.数据流程需要将数据类型结合起来定义流程处理节点
            3.数据处理流程、子流程、之间的关系和作业运行怎么处理
        
        当前版本设计：只按Flink的数据处理框架去适配、上述的数据处理功能服务主要是针对数据的变换进行封装；但是实际数据处理中
        还会涉及到数据的查询、过滤、聚合（窗口计算）、流关联等操作；对这些操作的融合，可以更多的考虑FlinkSQL的方式进行集成。
        
        
        
##### 系统存在的缺点
       1.因为数据转换不可能覆盖所有的具体场景、所以支持的有限度，一些复杂的业务计算转换还是需要手动编写代码。这个还是需要
         衡量自己应用的场景去制定数据转换方式实现 
         
         
##### 实践过程难点
      1.如何划分流处理框架下的根据描述节点生成对应的算子节点，这个实现就可以很好的将数据处理流程图----翻译为-->可执行程序
        （如何能覆盖80%的场景）,难以确定的点在于我不确定所实现的方式能否覆盖大部分场景!!，理论上是都可以解决的
        
        我们需要深入理解数据处理流程，主要就是数据的变换，然后就是数据的查询、聚合查询和入库
        

##### 2019.08.23下一阶段的计划

       想写点自己的代码，有时候不知道写点什么合适，实现一些基本的需求；也是业余的一种技能积累。在这一段的工作中，完成的还是
       可以的了。那么，下一阶段需要如何安排呢？
       
        所以后续想做两个事情：
        1.将 ETL和streamDB联系起来，统一操作开发平台（废弃）
        2.将数据业务流程图进行存储、可以清楚描述出流程的信息，帮助于业务理解。
        3.动态更新配置变量这个功能如何实现有两种方式：
           1.利用发布与订阅的机制(开启web server的方式直接否掉)
           2.结合广播的机制去更新参数、变量
           这两种方式的区别是：
           第一种方案需要独立的线程去监听事件然后更新配置，需要考虑变量同步的问题
           第二中方式使用流的connect方式去更新，并且不会存在变量同步的问题
           结合实际情况，我们选择第二种方案去更新配置
       
        完成的功能：
          1.数据转换流程中配置更新功能（更新数据处理流命令、例如动态刷新缓存、业务参数调整）
          2.子流程的调用功能实现
          
        收获：
           1.加强了一些对数据处理流程的理解和设计
              
       
        
 ##### 2019.09.16 
       
       1.需要注意的问题：影响程序运行，需要将>10ms耗时的command和message记录下来，进行日志分析
       2.正则表达式是一个比较耗时的坑，所以在写正则的时候都需要事先工具分析一下
       
       1.下一阶段想从一些消息处理、流式计算、存储等方面去
         探索一些三高的解决方案和搜罗一些问题的解析 
       2.还有想把特征抽取融入到数据处理流程中去
       3. 把数据质量、数据血缘关系建立起来
       4.思考一下数据仓库的设计
   
   ---------------------------------------------------------------------------------------------------------------    
       系统范围:
       
       业务范围：
       数据治理：采集治理、数据流程治理、数据存储治理、数据分析治理、
       
       1.采集治理包括对采集器的管理、升级、配置、采集方式、
       2.数据处理流程，数据汇聚、数据解析、富化计算、衍生数据、中间数据
       
       
  ##### 2019.09.26 想法和思路更新
       系统业务更新：
       在整个数据治理系统我们从系统架构层面可以分解为：
          业务数据源管理配置
          数据处理分析流程管理配置
          数据查询存储管理配置
          业务数据报表展示 
          
       以数据处理流程为中心去建立整个数据计算系统，用户可以通过定义一条数据处理流程，就可以直接启动该数据
       业务处理的任务。
      
      1.建立数据流程任务的配置和任务启动之间的联系,运行任务可以从第三方配置中心加载配置。其中当前版本实现的方式
        是在运行Job之前将配置生成配置文件打入需要启动的jar包中，后续对Flink改造熟悉了，可以在其运行时获取配置
      2.任务级别或者ETL计算级别的事件监听的实现
       
       技术架构上的更新：
       1.支持异步ETL计算数据转换的算子
       2.每一个ETL计算的命令可以定义输入输出字段
          
      
                      
        
        
        
            
          
           
           
          
          
        
        
        
        
                